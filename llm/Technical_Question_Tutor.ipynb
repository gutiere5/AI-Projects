{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "145c6758-291f-4b92-be2f-913136deaec2",
   "metadata": {},
   "source": [
    "# Technical Question Tutor\n",
    "To demostrate your familiarity with OpenAI API, and Ollama, build a tool that takes a technical question, and responds with an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33c10413-fd82-4b99-a6ff-ffee3ebfe941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0828c2e0-7efd-4b17-825f-1e52ce548ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "349b4a4d-fb24-472c-9bdb-eb9de51aab03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good so far\n"
     ]
    }
   ],
   "source": [
    "# Set up Environment\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good so far\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key?\")\n",
    "    \n",
    "MODEL = 'gpt-4o-mini'\n",
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbd933cc-7f3b-4967-bad2-159a104bf98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a specialized AI designed to help users understand \\\n",
    "complex technical topics. Follow these rules for every user request: \\\n",
    "1.  **Analyze the technical question.** Break down the core concept the user\\\n",
    "is asking about. \\\n",
    "2.  **Provide a concise explanation.** Keep the answer direct and easy to \\\n",
    "understand. Avoid jargon whenever possible.\\\n",
    "3.  **Create a simple, practical example.** The example should be in a clear \\\n",
    "code block or plain text that demonstrates the concept. \\\n",
    "4.  **Format the entire response using markdown.** Use headings, bolding, and \\\n",
    "code blocks to improve readability. \\\n",
    "Your primary goal is to provide helpful, actionable knowledge without \\\n",
    "unnecessary conversation or filler.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb1ce75-ebef-4584-ba3b-b9e40a5bf600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The question to ask \n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cff1c7-35dd-42b6-ac2d-b9793bf08fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompts(question):\n",
    "    user_prompt = f\"\"\"\n",
    "    I have a technical question about the following code snippet. Can you please provide a short explanation and a simple example of how it works?\n",
    "    \n",
    "    My question is:\n",
    "    {question}\n",
    "    \"\"\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6f0b74-408b-4664-929f-a88b53fe8ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get gpt-40-mini to answer with streaming\n",
    "def prompt_question(question):\n",
    "    stream = openai.chat.completions.create(\n",
    "        model=MODEL_GPT,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompts(question)}\n",
    "        ],\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    response = \"\"\n",
    "    display_handle = display(Markdown(\"\"), display_id=True)\n",
    "    for chunk in stream:\n",
    "        response += chunk.choices[0].delta.content or ''\n",
    "        update_display(Markdown(response), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5a9c50-d232-4fe2-92ed-9c46ddf2b6f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt_question(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518aa819-3736-44f4-9651-9da81fb72ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Llama 3.2 to answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be931b93-1477-4bfd-9899-ae0c5ba6ab27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
